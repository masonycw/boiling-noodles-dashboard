import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import timedelta

def render_member_search(df_report, df_details, latest_dates=None):
    st.title("ğŸ‘¥ æœƒå“¡æ¶ˆè²»ç´€éŒ„æŸ¥è©¢")
    
    # Show Data Freshness Info
    if latest_dates is None:
        latest_dates = {}
        
    json_date = latest_dates.get('json', 'ç„¡è³‡æ–™')
    csv_rep_date = latest_dates.get('csv_report', 'ç„¡è³‡æ–™')
    csv_det_date = latest_dates.get('csv_details', 'ç„¡è³‡æ–™')
    inv_date = latest_dates.get('invoice', 'ç„¡è³‡æ–™')

    st.info(f"**æœ€æ–°ç³»çµ±è³‡æ–™ç¯„åœæç¤º**\n\n"
            f"ğŸ“¡ **Eats365 API (JSON)**: `{json_date}` ï½œ "
            f"ğŸ“Š **ç‡Ÿæ¥­æ—¥å ±è¡¨ (CSV)**: `{csv_rep_date}`\n\n"
            f"ğŸ›’ **äº¤æ˜“æ˜ç´° (CSV)**: `{csv_det_date}` ï½œ "
            f"ğŸ§¾ **ç™¼ç¥¨æ˜ç´° (CSV)**: `{inv_date}`\n\n"
            f"*(æœƒå“¡æœå°‹çµæœæ¥µåº¦ä¾è³´æ­·å²ç´€éŒ„ï¼Œè«‹ç¢ºèªä¸Šé¢æ‰€æœ‰æ‰‹å‹• CSV æª”æ¡ˆéƒ½å·²ä¸Šå‚³æ›´æ–°è‡³æœ€æ–°æ—¥æœŸ)*")


    
    col_id = 'Member_ID'
    col_name = 'customer_name'
    col_phone = 'member_phone'
    col_carrier = 'carrier_id'
    
    if col_id not in df_report.columns:
        st.error("è³‡æ–™ç¼ºå°‘ Member_ID æ¬„ä½ (è«‹ç¢ºèª Data Loader)")
        return

    # --- Search Interface ---
    st.subheader("ğŸ” æœå°‹æœƒå“¡")
    st.caption("è¼¸å…¥è³‡æ–™ (å§“å / é›»è©± / è¼‰å…·è™Ÿç¢¼)")
    search_term = st.text_input("é—œéµå­— (Keyword)", "")
    
    if search_term:
        s_clean = search_term.strip()
        
        # Search Matrix
        mask = pd.Series(False, index=df_report.index)
        
        # Search in Name
        if col_name in df_report.columns:
            mask |= df_report[col_name].astype(str).str.contains(s_clean, na=False, case=False)
        # Search in Phone
        if col_phone in df_report.columns:
            mask |= df_report[col_phone].astype(str).str.contains(s_clean, na=False)
        # Search in Carrier
        if col_carrier in df_report.columns:
            mask |= df_report[col_carrier].astype(str).str.contains(s_clean, na=False, case=False)
        # Search in Member ID
        mask |= df_report[col_id].astype(str).str.contains(s_clean, na=False, case=False)
             
        results = df_report[mask].copy()
        
        if not results.empty:
            # Display Candidates
            # Create a label for selection
            def make_label(row):
                n = str(row.get(col_name, ''))
                p = str(row.get(col_phone, ''))
                c = str(row.get(col_carrier, ''))
                mid = str(row.get(col_id, ''))
                
                label = f"{n if n!='nan' else '-'} / {p if p!='nan' else '-'} / {c if c!='nan' else '-'}"
                return f"{label} (ID: {mid})"

            # Deduplicate by Member ID
            unique_members = results.drop_duplicates(subset=[col_id]).copy()
            unique_members['Label'] = unique_members.apply(make_label, axis=1)
            
            sel_label = st.selectbox(f"æ‰¾åˆ° {len(unique_members)} ä½ç›¸é—œæœƒå“¡:", unique_members['Label'].tolist())
            
            # Retrieve Selected Member Data
            sel_mid_row = unique_members[unique_members['Label'] == sel_label].iloc[0]
            sel_mid = sel_mid_row[col_id]
            
            # Fetch all records for this Member ID
            mem_records = df_report[df_report[col_id] == sel_mid].copy()
            mem_records = mem_records.sort_values('Date_Parsed', ascending=False)
            
            # --- Personal Dashboard ---
            st.divider()
            st.subheader(f"ğŸ‘¤ æœƒå“¡æª”æ¡ˆ: {sel_label}")
            
            # Metrics
            total_spend = mem_records['total_amount'].sum()
            visits = mem_records['Date_Parsed'].dt.date.nunique()
            first_visit = mem_records['Date_Parsed'].min().date()
            last_visit = mem_records['Date_Parsed'].max().date()
            
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("ç¸½æ¶ˆè²»", f"${total_spend:,.0f}")
            c2.metric("ä¾†åº—æ¬¡æ•¸", f"{visits} æ¬¡")
            c3.metric("åˆæ¬¡ä¾†åº—", str(first_visit))
            c4.metric("æœ€è¿‘ä¾†åº—", str(last_visit))
            
            # Purchase History
            if st.toggle("ğŸ§¾ é–‹å•Ÿï¼šæ­·å²æ¶ˆè²»æ­·ç¨‹ (Purchase History)", value=False):
                hist_df = mem_records[['Date_Parsed', 'order_id', 'total_amount', 'order_type', 'customer_name']].copy()
                st.dataframe(hist_df.style.format({'total_amount': '${:,.0f}', 'Date_Parsed': '{:%Y-%m-%d %H:%M}'}), use_container_width=True)
            
            # Favorite Items (if details available)
            if not df_details.empty:
                # Need to link details via order_id
                mem_orders = mem_records['order_id'].unique()
                mem_details = df_details[df_details['order_id'].isin(mem_orders)].copy()
                
                if not mem_details.empty:
                    st.subheader("â¤ï¸ å–œå¥½å•†å“")
                    # Filter modifiers
                    if 'Is_Modifier' in mem_details.columns:
                        mem_details = mem_details[~mem_details['Is_Modifier']]
                        
                    fav_items = mem_details.groupby('item_name')['qty'].sum().reset_index().sort_values('qty', ascending=False).head(5)
                    st.bar_chart(fav_items.set_index('item_name'))
            
        else:
            st.warning("æŸ¥ç„¡è³‡æ–™")

def render_crm_analysis(df_report, df_details, df_crm, latest_dates=None):
    st.title("ğŸ†• æ–°èˆŠå®¢åˆ†æ (New vs Returning)")
    
    # Show Data Freshness Info
    if latest_dates is None:
        latest_dates = {}
        
    json_date = latest_dates.get('json', 'ç„¡è³‡æ–™')
    csv_rep_date = latest_dates.get('csv_report', 'ç„¡è³‡æ–™')
    csv_det_date = latest_dates.get('csv_details', 'ç„¡è³‡æ–™')
    inv_date = latest_dates.get('invoice', 'ç„¡è³‡æ–™')

    st.info(f"**æœ€æ–°ç³»çµ±è³‡æ–™ç¯„åœæç¤º**\n\n"
            f"ğŸ“¡ **Eats365 API (JSON)**: `{json_date}` ï½œ "
            f"ğŸ“Š **ç‡Ÿæ¥­æ—¥å ±è¡¨ (CSV)**: `{csv_rep_date}`\n\n"
            f"ğŸ›’ **äº¤æ˜“æ˜ç´° (CSV)**: `{csv_det_date}` ï½œ "
            f"ğŸ§¾ **ç™¼ç¥¨æ˜ç´° (CSV)**: `{inv_date}`\n\n"
            f"*(æ–°èˆŠå®¢èˆ‡æœƒå“¡åˆ¤å®šæ¥µåº¦ä¾è³´æ­·å²ç´€éŒ„ï¼Œè«‹ç¢ºèªä¸Šé¢æ‰€æœ‰æ‰‹å‹• CSV æª”æ¡ˆéƒ½å·²ä¸Šå‚³æ›´æ–°è‡³æœ€æ–°æ—¥æœŸ)*")


    with st.expander("â„¹ï¸ æ–°èˆŠå®¢èˆ‡éæœƒå“¡å®šç¾©èªªæ˜"):
        st.markdown("""
        * **æ–°å®¢ (New)**ï¼šåœ¨æ‚¨é¸æ“‡çš„å€é–“å…§ï¼Œè©²æœƒå“¡ç™¼ç”Ÿäº†ã€Œæ­·å²ä»¥ä¾†çš„ç¬¬ 1 æ¬¡ã€æ¶ˆè²»ã€‚
        * **èˆŠå®¢ (Returning)**ï¼šåœ¨æ‚¨é¸æ“‡çš„å€é–“å…§æœ‰æ¶ˆè²»ï¼Œä½†ä»–çš„ã€Œæ­·å²ç¬¬ 1 æ¬¡ã€æ¶ˆè²»ç™¼ç”Ÿåœ¨é€™å€‹å€é–“ä¹‹å‰ã€‚
        * **éæœƒå“¡ (Non-member)**ï¼šæœ¬æ¬¡äº¤æ˜“æœªç¶å®šæœƒå“¡é›»è©±æˆ–è¼‰å…·ã€‚
        """)
    
    col_id = 'Member_ID'
    if col_id not in df_report.columns:
        df_report[col_id] = None
        
    df = df_report.copy()
    # Treat NaN as non-member
    df[col_id] = df[col_id].fillna('éæœƒå“¡')
    
    st.divider()
    
    st.subheader("ğŸ—“ï¸ å–®æœŸç¶œåˆåˆ†æå€é–“")
    from .utils import render_date_filter
    s_date, e_date = render_date_filter("crm_tab1", "é€™å€‹æœˆ (This Month)")
    
    start_ts = pd.Timestamp(s_date)
    end_ts = pd.Timestamp(e_date)
    
    period_txs = df[(df['Date_Parsed'] >= start_ts) & (df['Date_Parsed'] <= end_ts)].copy()
    
    if period_txs.empty:
        st.warning("æ­¤å€é–“ç„¡äº¤æ˜“è³‡æ–™")
        return
        
    # Process Members
    member_mask = df[col_id] != 'éæœƒå“¡'
    df_members = df[member_mask]
    
    # Calculate First Visit for ALL valid members
    member_first_visit = df_members.groupby(col_id)['Date_Parsed'].min().reset_index()
    member_first_visit.columns = [col_id, 'First_Visit_Date']
    
    # Map back to period transactions to determine type
    period_txs = period_txs.merge(member_first_visit, on=col_id, how='left')
    
    period_txs['Date_Only'] = period_txs['Date_Parsed'].dt.date
    
    def determine_type(row):
        if row[col_id] == 'éæœƒå“¡':
            return 'éæœƒå“¡ (Non-member)'
        if pd.isna(row['First_Visit_Date']):
            # Should not happen given we only merge members that exist, but failsafe
            return 'éæœƒå“¡ (Non-member)'
        if row['First_Visit_Date'] >= start_ts:
            return 'æ–°å®¢ (New)'
        return 'èˆŠå®¢ (Returning)'
        
    period_txs['User_Type'] = period_txs.apply(determine_type, axis=1)
    
    # Create Visit_ID to deduplicate same-day member visits
    def get_visit_id(row):
        if row['User_Type'] == 'éæœƒå“¡ (Non-member)':
            return str(row['order_id'])
        else:
            return f"{row[col_id]}_{row['Date_Only']}"
            
    period_txs['Visit_ID'] = period_txs.apply(get_visit_id, axis=1)
    
    # Stats
    type_counts = period_txs.groupby('User_Type')['Visit_ID'].nunique()
    
    rev_by_type = period_txs.groupby('User_Type').agg(
        Total_Revenue=('total_amount', 'sum'),
        Tx_Count=('Visit_ID', 'nunique')
    ).reset_index()
    
    # Map safely
    def get_stat(df, c, v):
        res = df.loc[df['User_Type'] == c, v]
        return res.values[0] if not res.empty else 0
        
    new_rev = get_stat(rev_by_type, 'æ–°å®¢ (New)', 'Total_Revenue')
    ret_rev = get_stat(rev_by_type, 'èˆŠå®¢ (Returning)', 'Total_Revenue')
    non_rev = get_stat(rev_by_type, 'éæœƒå“¡ (Non-member)', 'Total_Revenue')
    
    new_txs = get_stat(rev_by_type, 'æ–°å®¢ (New)', 'Tx_Count')
    ret_txs = get_stat(rev_by_type, 'èˆŠå®¢ (Returning)', 'Tx_Count')
    non_txs = get_stat(rev_by_type, 'éæœƒå“¡ (Non-member)', 'Tx_Count')
    
    total_rev = period_txs['total_amount'].sum()
    total_txs = period_txs['Visit_ID'].nunique()

    m1, m2, m3, m4 = st.columns(4)
    m1.metric("ğŸ‘¥ ç¸½ä¾†åº—å®¢çµ„ (Visit_ID)", f"{total_txs:,.0f} çµ„", help="æ ¹æ“šè¨‚å–®IDæˆ–æœƒå“¡æ¯æ—¥è¨ˆç®—çš„ä¸é‡è¤‡ä¾†è¨ªæ•¸")
    m2.metric("ğŸ†• æ–°å®¢ç‡Ÿæ”¶ä½”æ¯”", f"${new_rev:,.0f}", delta=f"ä½”æ¯” {new_rev/total_rev:.1%}" if total_rev else "ä½”æ¯” 0%", delta_color="off", help="æ–°å®¢ç‡Ÿæ”¶ä½”ã€Œå…¨åº—ç¸½ç‡Ÿæ”¶ã€(å«éæœƒå“¡) çš„æ¯”ä¾‹")
    m3.metric("ğŸ¤ èˆŠå®¢ç‡Ÿæ”¶ä½”æ¯”", f"${ret_rev:,.0f}", delta=f"ä½”æ¯” {ret_rev/total_rev:.1%}" if total_rev else "ä½”æ¯” 0%", delta_color="off", help="èˆŠå®¢ç‡Ÿæ”¶ä½”ã€Œå…¨åº—ç¸½ç‡Ÿæ”¶ã€(å«éæœƒå“¡) çš„æ¯”ä¾‹")
    m4.metric("â“ éæœƒå“¡ç‡Ÿæ”¶ä½”æ¯”", f"${non_rev:,.0f}", delta=f"ä½”æ¯” {non_rev/total_rev:.1%}" if total_rev else "ä½”æ¯” 0%", delta_color="off", help="éæœƒå“¡ç‡Ÿæ”¶ä½”ã€Œå…¨åº—ç¸½ç‡Ÿæ”¶ã€çš„æ¯”ä¾‹")
    
    total_active = new_txs + ret_txs # Approximation or actual if 1 tx per member average? No, let's use actual:
    member_txs = period_txs[period_txs['User_Type'] != 'éæœƒå“¡ (Non-member)']
    total_active = member_txs[col_id].nunique() if not member_txs.empty else 0
    new_active = type_counts.get('æ–°å®¢ (New)', 0)
    ret_active = type_counts.get('èˆŠå®¢ (Returning)', 0)

    m1, m2, m3, m4 = st.columns(4)
    m1.metric("ğŸ‘¤ ç¸½æ´»èºæœƒå“¡", f"{total_active:,.0f} äºº", help="å€é–“å…§æœ‰æ¶ˆè²»ç´€éŒ„çš„ç¨ç«‹æœƒå“¡æ•¸")
    m2.metric("ğŸ†• æ–°æœƒå“¡æ•¸", f"{new_active:,.0f} äºº", delta=f"ä½”æ¯” {new_active/total_active:.1%}" if total_active else "ä½”æ¯” 0%", delta_color="off", help="å€é–“å…§ç™¼ç”Ÿæ­·å²é¦–æ¬¡æ¶ˆè²»çš„ç¨ç«‹æœƒå“¡æ•¸")
    m3.metric("ğŸ’¸ æ–°å®¢æœƒå“¡å…§è²¢ç»", f"${new_rev:,.0f}", delta=f"ä½”æ¯” {new_rev/(new_rev+ret_rev):.1%}" if (new_rev+ret_rev) else "ä½”æ¯” 0%", delta_color="off", help="æ–°å®¢ç‡Ÿæ”¶ä½”ã€Œæ‰€æœ‰æœƒå“¡ç¸½ç‡Ÿæ”¶ã€(æ’é™¤éæœƒå“¡) çš„æ¯”ä¾‹")
    m4.metric("ğŸ’° èˆŠå®¢æœƒå“¡å…§è²¢ç»", f"${ret_rev:,.0f}", delta=f"ä½”æ¯” {ret_rev/(new_rev+ret_rev):.1%}" if (new_rev+ret_rev) else "ä½”æ¯” 0%", delta_color="off", help="èˆŠå®¢ç‡Ÿæ”¶ä½”ã€Œæ‰€æœ‰æœƒå“¡ç¸½ç‡Ÿæ”¶ã€(æ’é™¤éæœƒå“¡) çš„æ¯”ä¾‹")
    
    st.divider()

    c1, c2 = st.columns(2)
    with c1:
        st.subheader("ğŸ‘¥ å®¢ç¾¤ç­†æ•¸åˆ†ä½ˆ (åŒæ—¥åŒåç‚ºä¸€ç­†)")
        fig = px.pie(values=type_counts.values, names=type_counts.index, title="æœŸé–“ä¾†è¨ªä½”æ¯” (å«éæœƒå“¡)", hole=0.4)
        st.plotly_chart(fig, use_container_width=True)
        
    with c2:
        st.subheader("ğŸ’³ å¹³å‡å®¢å–®åƒ¹ (Avg Check by Type)")
        avg_df = pd.DataFrame([
            {'User_Type': 'æ–°å®¢ (New)', 'Avg_Spend': new_rev / new_txs if new_txs else 0},
            {'User_Type': 'èˆŠå®¢ (Returning)', 'Avg_Spend': ret_rev / ret_txs if ret_txs else 0},
            {'User_Type': 'éæœƒå“¡ (Non-member)', 'Avg_Spend': non_rev / non_txs if non_txs else 0}
        ])
        fig2 = px.bar(avg_df, x='User_Type', y='Avg_Spend', title="å¹³å‡å®¢å–®åƒ¹æ¯”è¼ƒ", text_auto='.0f', color='User_Type')
        st.plotly_chart(fig2, use_container_width=True)
        
    st.divider()
    
    # Popular Items Section
    st.subheader("ğŸ† å„é¡å®¢ç¾¤ç†±é–€é¤é»åˆ†æ")
    st.caption("ä¾æ“šä¸»é£ŸéŠ·é‡æ’åº (é¡¯ç¤º Top 5)")
    
    # Merge User_Type into details
    if not df_details.empty and not period_txs.empty:
        # Get mapping of order_id to User_Type
        order_type_map = period_txs[['order_id', 'User_Type']].drop_duplicates()
        curr_details = df_details[
            (df_details['Date_Parsed'] >= start_ts) & 
            (df_details['Date_Parsed'] <= end_ts) & 
            (df_details['Is_Main_Dish'] == True)
        ].merge(order_type_map, on='order_id', how='inner')
        
        if not curr_details.empty:
            types_to_show = ['æ–°å®¢ (New)', 'èˆŠå®¢ (Returning)', 'éæœƒå“¡ (Non-member)']
            cols = st.columns(3)
            
            for i, u_type in enumerate(types_to_show):
                with cols[i]:
                    st.markdown(f"**{u_type}**")
                    df_u = curr_details[curr_details['User_Type'] == u_type]
                    if not df_u.empty:
                        top_items = df_u.groupby('item_name')['qty'].sum().reset_index().sort_values('qty', ascending=False).head(5)
                        # Minimalist bar chart
                        st.dataframe(top_items.rename(columns={'item_name': 'é¤é»', 'qty': 'æ•¸é‡'}).set_index('é¤é»'), use_container_width=False)
                    else:
                        st.caption("ç„¡è³‡æ–™")
    else:
        st.info("ç„¡æ³•è¼‰å…¥æ˜ç´°è³‡æ–™é€²è¡Œç†±é–€å•†å“åˆ†æã€‚")
        
    st.divider()
    
    # Time Series: New vs Returning over time
    st.subheader("ğŸ“ˆ æ—¥å¸¸å®¢ç¾¤ä¾†åº—è¶¨å‹¢")
    
    daily_type = period_txs.groupby(['Date_Only', 'User_Type'])['Visit_ID'].nunique().reset_index()
    daily_type.rename(columns={'Visit_ID': 'Visits'}, inplace=True)
    
    fig_time = px.bar(daily_type, x='Date_Only', y='Visits', color='User_Type', title="æ¯æ—¥å®¢ç¾¤ä¾†è¨ªæ•¸ (åŒæ—¥è¦–ç‚º 1 ç­†)", barmode='stack')
    st.plotly_chart(fig_time, use_container_width=True)

    ###################################################################
    #                     (Code moved to bottom of file)
    ###################################################################

        
    st.divider()
    
    # Retention / Frequency
    st.subheader("ğŸ“Š æœŸé–“å›è¨ªé »ç‡ (åƒ…é™æœƒå“¡)")
    member_only_txs = period_txs[period_txs['User_Type'] != 'éæœƒå“¡ (Non-member)']
    freq = member_only_txs.groupby(col_id)['Visit_ID'].nunique().reset_index()
    freq['Frequency'] = pd.cut(freq['Visit_ID'], bins=[0, 1, 2, 5, 100], labels=['1æ¬¡', '2æ¬¡', '3-5æ¬¡', '6æ¬¡+'])
    
    # Split frequency by User Type to see if new users ever come back twice in the same period
    user_type_map = member_only_txs[[col_id, 'User_Type']].drop_duplicates()
    freq = freq.merge(user_type_map, on=col_id, how='left')
    freq_summary = freq.groupby(['User_Type', 'Frequency']).size().reset_index(name='Count')
    
    fig_freq = px.bar(freq_summary, x='Frequency', y='Count', color='User_Type', barmode='group', title="æœŸé–“å…§æ¶ˆè²»æ¬¡æ•¸åˆ†ä½ˆ")
    st.plotly_chart(fig_freq, use_container_width=True)

    st.divider()
    
    # RFM Analysis
    st.subheader("ğŸ¯ å€é–“å…§å®¢ç¾¤ç´°ç¯€ (RFM Scatter Plot)")
    st.caption("åŸºæ–¼æ‚¨é¸æ“‡çš„æ—¥æœŸå€é–“ï¼Œè¨ˆç®—æ´»èºæœƒå“¡çš„ R (æœ€è¿‘ä¸€æ¬¡æ¶ˆè²»è·ä»Š)ã€F (å€é–“å…§ä¾†åº—æ¬¡æ•¸)ã€M (å€é–“å…§ç´¯ç©æ¶ˆè²»)ã€‚")
    
    with st.expander("â„¹ï¸ RFM å®¢ç¾¤å®šç¾©èªªæ˜"):
        st.markdown("""
        * ğŸŸ¢ **Champions (ä¸»åŠ›å¸¸å®¢)**ï¼šå€é–“å…§ä¾†åº— â‰¥ 3 æ¬¡ï¼Œä¸”è¿‘æœŸæœ‰å›è¨ªã€‚
        * ğŸŸ  **Potential (æ½›åŠ›æ–°æ˜Ÿ)**ï¼šå€é–“å…§ä¾†åº— 2 æ¬¡ï¼Œä¸”è¿‘æœŸæœ‰å›è¨ªã€‚
        * ğŸ”´ **New (æ–°å®¢)**ï¼šå€é–“å…§ä¾†åº— 1 æ¬¡ï¼Œä¸”è¿‘æœŸæ‰ä¾†è¨ªã€‚
        * ğŸ©µ **At Risk (æµå¤±é è­¦)**ï¼šå€é–“å…§ä¾†åº— â‰¥ 2 æ¬¡ï¼Œä½†è¿‘æœŸæœªå†å›è¨ªã€‚
        * ğŸ”µ **One-time (ä¸€æ¬¡å®¢)**ï¼šå€é–“å…§åªä¾†åº— 1 æ¬¡ï¼Œä¸”è¿‘æœŸæœªå†å›è¨ªã€‚
        
        > ğŸ’¡ **ã€Œè¿‘æœŸå›è¨ªã€åŸºæº–å¤©æ•¸**ï¼šè‹¥æ‚¨çš„æŸ¥è©¢å€é–“å¤§æ–¼ 28 å¤©ï¼Œåˆ¤å®šæ¨™æº–ç‚ºã€Œå€é–“å¤©æ•¸çš„ä¸€åŠã€ï¼›è‹¥æŸ¥è©¢å€é–“è¼ƒçŸ­ï¼Œå‰‡å›ºå®šä»¥è·ä»Šã€Œ14 å¤©å…§ã€ç‚ºç•Œã€‚
        """)
    
    interval_txs = period_txs[period_txs[col_id] != 'éæœƒå“¡'].copy()
    
    if not interval_txs.empty:
        # Calculate R, F, M
        # Calculate R, F, M
        rfm = interval_txs.groupby(col_id).agg(
            Last_Purchase=('Date_Parsed', 'max'),
            Frequency=('Visit_ID', 'nunique'),
            Monetary=('total_amount', 'sum')
        ).reset_index()
        
        # Merge Global First Visit Date to show how "old" the customer is
        global_first = df_report[df_report[col_id] != 'éæœƒå“¡'].groupby(col_id)['Date_Parsed'].min().reset_index(name='First_Visit_Global')
        rfm = rfm.merge(global_first, on=col_id, how='left')
        rfm['Days_Since_First_Visit'] = (pd.Timestamp(end_ts.date()) - pd.to_datetime(rfm['First_Visit_Global']).dt.normalize()).dt.days
        rfm['First_Visit_Str'] = pd.to_datetime(rfm['First_Visit_Global']).dt.strftime('%Y-%m-%d')
        
        # Merge Global Frequency to show all-time visits
        global_freq = df_report[df_report[col_id] != 'éæœƒå“¡'].groupby(col_id)['order_id'].nunique().reset_index(name='Frequency_Global')
        rfm = rfm.merge(global_freq, on=col_id, how='left')
        
        # Calculate Recency in days (against the end of the selected period)
        rfm['Recency'] = (pd.Timestamp(end_ts.date()) - pd.to_datetime(rfm['Last_Purchase']).dt.normalize()).dt.days
        rfm['Recency'] = rfm['Recency'].clip(lower=0)
        
        interval_days = max((end_ts.date() - start_ts.date()).days, 1)
        r_thresh = interval_days / 2 if interval_days >= 28 else 14
        
        def segment_rfm(row):
            f = row['Frequency']
            r = row['Recency']
            
            if f >= 3:
                return "Champions (ä¸»åŠ›å¸¸å®¢)" if r <= r_thresh else "At Risk (æµå¤±é è­¦)"
            elif f == 2:
                return "Potential (æ½›åŠ›æ–°æ˜Ÿ)" if r <= r_thresh else "At Risk (æµå¤±é è­¦)"
            else:
                return "New (æ–°å®¢)" if r <= r_thresh else "One-time (ä¸€æ¬¡å®¢)"
                
        rfm['Segment'] = rfm.apply(segment_rfm, axis=1)
        
        color_map = {
            "Champions (ä¸»åŠ›å¸¸å®¢)": "#7FCCB5",
            "Potential (æ½›åŠ›æ–°æ˜Ÿ)": "#FDD1C9",
            "New (æ–°å®¢)": "#FF7B72",
            "At Risk (æµå¤±é è­¦)": "#A5D8FF",
            "One-time (ä¸€æ¬¡å®¢)": "#5B96DB"
        }
        cat_order = list(color_map.keys())
        
        fig_scatter = px.scatter(
            rfm, 
            x='Recency', 
            y='Frequency', 
            size='Monetary', 
            color='Segment', 
            hover_name=col_id,
            hover_data={
                'First_Visit_Str': True,
                'Days_Since_First_Visit': True,
                'Frequency_Global': True,
                'Recency': True, 
                'Frequency': True,
                'Segment': False, 
                'First_Visit_Global': False
            },
            category_orders={"Segment": cat_order},
            color_discrete_map=color_map,
            title="RFM åˆ†ä½ˆ (X=å¤©æ•¸æœªè¨ª, Y=æ¶ˆè²»æ¬¡æ•¸, å¤§å°=æ¶ˆè²»é¡)",
            labels={
                'Recency': 'Recency (å¤©æ•¸æœªè¨ª - è¶Šå°è¶Šå¥½)',
                'Frequency': 'Frequency (å€é–“ä¾†è¨ªæ¬¡æ•¸)',
                'First_Visit_Str': 'æ­·å²é¦–è¨ªæ—¥',
                'Days_Since_First_Visit': 'æˆç‚ºæœƒå“¡å¤©æ•¸',
                'Frequency_Global': 'æ­·å²ç¸½ä¾†è¨ªæ¬¡æ•¸'
            },
            size_max=30
        )
        if st.toggle("ğŸ“Š é–‹å•Ÿï¼šRFM æœƒå“¡åˆ†ä½ˆæ•£ä½ˆåœ– (è€—è²»é‹ç®—è³‡æº)", value=False):
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        st.divider()
        
        seg_counts = rfm['Segment'].value_counts().reset_index()
        seg_counts.columns = ['æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', 'äººæ•¸']
        
        # Avg M per segment
        seg_m = rfm.groupby('Segment')['Monetary'].mean().reset_index()
        seg_counts = seg_counts.merge(seg_m, left_on='æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', right_on='Segment')
        
        col_rfm1, col_rfm2 = st.columns([1, 1])
        with col_rfm1:
            fig_rfm = px.pie(
                seg_counts, names='æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', values='äººæ•¸', 
                title="å€é–“ RFM æœƒå“¡åˆ†ç¾¤ä½”æ¯”", hole=0.3,
                color='æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', color_discrete_map=color_map,
                category_orders={"æœƒå“¡åƒ¹å€¼åˆ†ç¾¤": cat_order}
            )
            st.plotly_chart(fig_rfm, use_container_width=True)
            
        with col_rfm2:
            fig_rfm2 = px.bar(
                seg_counts, x='æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', y='Monetary', 
                title="å„ç¾¤é«”å¹³å‡å€é–“è²¢ç» ($)", text_auto='.0f',
                color='æœƒå“¡åƒ¹å€¼åˆ†ç¾¤', color_discrete_map=color_map,
                category_orders={"æœƒå“¡åƒ¹å€¼åˆ†ç¾¤": cat_order}
            )
            fig_rfm2.update_layout(showlegend=False)
            st.plotly_chart(fig_rfm2, use_container_width=True)

    ###################################################################
    #                     Rolling Trend Section
    ###################################################################
    st.divider()
    
    st.subheader("ğŸ—“ï¸ é•·æœŸèµ°å‹¢è§€å¯Ÿå€é–“")
    from .utils import render_date_filter
    s_date_t2, e_date_t2 = render_date_filter("crm_trend", "é€™å€‹æœˆ (This Month)")
    
    start_ts_t2 = pd.Timestamp(s_date_t2)
    end_ts_t2 = pd.Timestamp(e_date_t2)
    
    # Historical Rolling Trend (Excluding Closures)
    st.subheader("ğŸ“Š æ­·å²å®¢ç¾¤ç‡Ÿæ”¶èµ°å‹¢ (éå» 28 ç‡Ÿæ¥­æ—¥ç§»å‹•ç¸½å’Œå¹³æ»‘)")
    st.caption("è‡ªå‹•æ’é™¤åº—ä¼‘èˆ‡ç„¡ç‡Ÿæ”¶æ—¥ï¼Œæ¯ä¸€é»ä»£è¡¨ã€ŒåŒ…å«ç•¶æ—¥åœ¨å…§çš„éå» 28 å€‹å¯¦éš›ç‡Ÿæ¥­æ—¥ã€çš„å®¢ç¾¤ç‡Ÿæ”¶**ç¸½å’Œ**ã€‚")
    
    if df_crm.empty:
        df_crm = pd.DataFrame(columns=['Date_Parsed', 'User_Type', 'total_amount', 'Active_Members'])
        
    df_crm['Date_Only'] = df_crm['Date_Parsed'].dt.date
    daily_total = df_crm.groupby('Date_Only')['total_amount'].sum().reset_index()
    active_days = daily_total[daily_total['total_amount'] > 0]['Date_Only'].sort_values().unique()
    
    if len(active_days) > 0:
        daily_rev = df_crm.groupby(['Date_Only', 'User_Type'])['total_amount'].sum().unstack(fill_value=0).reset_index()
        
        for c in ['æ–°å®¢ (New)', 'èˆŠå®¢ (Returning)', 'éæœƒå“¡ (Non-member)']:
            if c not in daily_rev.columns: daily_rev[c] = 0
            
        daily_rev = daily_rev[daily_rev['Date_Only'].isin(active_days)].sort_values('Date_Only')
        
        rolling_df = daily_rev.copy()
        rolling_df['æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] = rolling_df['æ–°å®¢ (New)'].rolling(window=28, min_periods=1).sum()
        rolling_df['èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] = rolling_df['èˆŠå®¢ (Returning)'].rolling(window=28, min_periods=1).sum()
        rolling_df['éæœƒå“¡ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] = rolling_df['éæœƒå“¡ (Non-member)'].rolling(window=28, min_periods=1).sum()
        
        # Calculate Percentage Shares ONLY based on Member Revenue (Total = New + Returning)
        rolling_df['ç´”æœƒå“¡ç¸½å’Œ (28æ—¥)'] = rolling_df['æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] + rolling_df['èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)']
        rolling_df['æœƒå“¡ç¸½å’Œ_Safe'] = rolling_df['ç´”æœƒå“¡ç¸½å’Œ (28æ—¥)'].replace(0, np.nan)
        
        rolling_df['æ–°å®¢æœƒå“¡å…§è²¢ç» (28æ—¥)'] = rolling_df['æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] / rolling_df['æœƒå“¡ç¸½å’Œ_Safe']
        rolling_df['èˆŠå®¢æœƒå“¡å…§è²¢ç» (28æ—¥)'] = rolling_df['èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)'] / rolling_df['æœƒå“¡ç¸½å’Œ_Safe']
        
        mask_period = (pd.to_datetime(rolling_df['Date_Only']) >= start_ts_t2) & (pd.to_datetime(rolling_df['Date_Only']) <= end_ts_t2)
        plot_df = rolling_df.loc[mask_period].copy()
        
        if not plot_df.empty:
            recent_stats = rolling_df[pd.to_datetime(rolling_df['Date_Only']) <= end_ts_t2]
            
            if not recent_stats.empty:
                latest_row = recent_stats.iloc[-1]
                latest_date_str = latest_row['Date_Only'].strftime('%Y-%m-%d')
                
                n_rev28 = latest_row['æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)']
                r_rev28 = latest_row['èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)']
                nm_rev28 = latest_row['éæœƒå“¡ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)']
                total_rev28 = n_rev28 + r_rev28 + nm_rev28
                member_rev28 = n_rev28 + r_rev28
                
                idx = np.where(active_days == latest_row['Date_Only'])[0]
                if len(idx) > 0:
                    end_idx = idx[0]
                    start_idx = max(0, end_idx - 27)
                    window_days_crm = active_days[start_idx : end_idx + 1]
                    t2_txs = df_crm[(df_crm['Date_Parsed'].dt.date.isin(window_days_crm)) & (df_crm['User_Type'] != 'éæœƒå“¡ (Non-member)')]
                    unique_members_28d = t2_txs['Active_Members'].sum()
                else:
                    unique_members_28d = 0
                
                st.markdown(f"**ğŸ“Œ åŸºæº–æ—¥ç‹€æ…‹å¿«ç…§** (ä»¥ `{latest_date_str}` å¾€å‰æ¨ç®— 28 å¯¦éš›ç‡Ÿæ¥­æ—¥)")
                m1, m2, m3, m4 = st.columns(4)
                m1.metric("ğŸ‘¤ 28ç‡Ÿæ¥­æ—¥ç¸½æ´»èºæœƒå“¡", f"{unique_members_28d:,.0f} äºº")
                m2.metric("ğŸ†• æ–°å®¢ç‡Ÿæ”¶è²¢ç» (28æ—¥)", f"${n_rev28:,.0f}", f"ä½”ç¸½ç‡Ÿæ”¶ {n_rev28/total_rev28:.1%}" if total_rev28 else "0%", delta_color="off")
                m3.metric("ğŸ¤ èˆŠå®¢ç‡Ÿæ”¶è²¢ç» (28æ—¥)", f"${r_rev28:,.0f}", f"ä½”ç¸½ç‡Ÿæ”¶ {r_rev28/total_rev28:.1%}" if total_rev28 else "0%", delta_color="off")
                m4.metric("â“ éæœƒå“¡ç‡Ÿæ”¶ä½”æ¯” (28æ—¥)", f"${nm_rev28:,.0f}", f"ä½”ç¸½ç‡Ÿæ”¶ {nm_rev28/total_rev28:.1%}" if total_rev28 else "0%", delta_color="off")
                
            st.divider()    
            
            st.divider()    
            
            # Create figure with secondary y-axis
            fig_rolling = make_subplots(specs=[[{"secondary_y": True}]])
            
            color_map = {
                'æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)': '#FF7B72',
                'èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)': '#7FCCB5',
                'éæœƒå“¡ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)': '#C9D1D9',
                'èˆŠå®¢æœƒå“¡å…§è²¢ç» (28æ—¥)': '#7FCCB5' # We will use a different style for this line
            }
            
            # --- Primary Y-Axis (Absolute Revenue) ---
            val_vars_abs = ['æ–°å®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)', 'èˆŠå®¢ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)', 'éæœƒå“¡ç‡Ÿæ”¶ç¸½å’Œ (28æ—¥)']
            for col in val_vars_abs:
                fig_rolling.add_trace(
                    go.Scatter(
                        x=plot_df['Date_Only'], 
                        y=plot_df[col], 
                        name=col,
                        line=dict(color=color_map[col], width=3),
                        hovertemplate='<b>æ—¥æœŸ</b>: %{x}<br><b>' + col + '</b>: %{y:$,.0f}<extra></extra>'
                    ),
                    secondary_y=False,
                )
                
            # --- Secondary Y-Axis (Percentage Share) ---
            fig_rolling.add_trace(
                go.Scatter(
                    x=plot_df['Date_Only'], 
                    y=plot_df['èˆŠå®¢æœƒå“¡å…§è²¢ç» (28æ—¥)'], 
                    name='èˆŠå®¢æœƒå“¡å…§è²¢ç»ä½”æ¯” (28æ—¥)',
                    line=dict(color='#F2C94C', width=3, dash='dot'), # Distinct Yellow/Gold dotted line for percentage
                    hovertemplate='<b>æ—¥æœŸ</b>: %{x}<br><b>èˆŠå®¢ä½”æ¯”</b>: %{y:.1%}<extra></extra>'
                ),
                secondary_y=True,
            )
            
            # --- Layout Configuration ---
            fig_rolling.update_layout(
                title="å®¢ç¾¤ 28 ç‡Ÿæ¥­æ—¥æ»¾å‹•ç¸½ç‡Ÿæ”¶èˆ‡èˆŠå®¢ä½”æ¯”è¶¨å‹¢",
                hovermode="x unified", # Shows all tooltip data at once for the given x-axis hovered date
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
            )
            
            # Set y-axes titles/formatting
            fig_rolling.update_yaxes(title_text="28ç‡Ÿæ¥­æ—¥ç¸½ç‡Ÿæ”¶", secondary_y=False)
            fig_rolling.update_yaxes(title_text="èˆŠå®¢æœƒå“¡å…§è²¢ç»ä½”æ¯”", tickformat='.1%', secondary_y=True, range=[0, 1.05]) # Fix max to 105% context so the line doesn't hit the absolute top
            
            if st.toggle("ğŸ“Š é–‹å•Ÿï¼šè©³ç´°æ»¾å‹•è¶¨å‹¢åœ–è¡¨ (è€—è²»é‹ç®—è³‡æº)", value=False):
                st.plotly_chart(fig_rolling, use_container_width=True)
        else:
            st.info("è©²å€é–“ä¸¦ç„¡è¶³å¤ çš„ç‡Ÿæ¥­æ—¥å¯ä»¥é¡¯ç¤ºè¶¨å‹¢ã€‚")
    else:
        st.info("è³‡æ–™åº«ä¸­ç„¡å¤§æ–¼ 0 çš„ç‡Ÿæ¥­æ—¥ç´€éŒ„ã€‚")
